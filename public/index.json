[
{
	"uri": "/6-lambda/6.1-role/",
	"title": "Creating a Role",
	"tags": [],
	"description": "",
	"content": "Creating a Role in IAM To begin, let\u0026rsquo;s create a role in IAM. Enter IAM in the service search bar on the AWS Console and select IAM. In the left navigation pane, choose Roles, then select Create role. Choose AWS service and Lambda. Then, click Next. Search for the following keywords and select the corresponding policies: CloudWatch: CloudWatchFullAccess S3: AmazonS3FullAccess DynamoDB: AmazonDynamoDBFullAccess Kinesis: AmazonKinesisFullAccess Note: In the context of this lab, for simplicity and convenience, we are using a single role for both Lambda Functions and granting FullAccess permissions to all services. However, in a real-world scenario, you should configure roles with specific policies to ensure security and proper access control.\nIn actual deployments, breaking down roles and setting up specific policies will help limit access rights based on tasks and security risks. This helps protect your resources from unauthorized access and potential security vulnerabilities.\nOnce all policies are added, click Next.\nEnter the Role name as RoleLab2. Review the settings and choose Create role. "
},
{
	"uri": "/5-databases/5.1-dynamodb/",
	"title": "Initializing DynamoDB",
	"tags": [],
	"description": "",
	"content": "Initializing DynamoDB Enter DynamoDB in the service search bar on the AWS Console, then select DynamoDB.\nChoose Create table.\nIn the Table name field, enter Data-Kinesis. For the Partition key, enter timestamp. Leave the rest as default.\nSelect Create table.\nWait for about 2 minutes until the status changes to Active, indicating completion.\n"
},
{
	"uri": "/5-databases/5.2-s3/",
	"title": "Initializing S3",
	"tags": [],
	"description": "",
	"content": "Initializing S3 Before proceeding, we need to create an IAM role:\nEnter S3 in the service search bar on the AWS Console, then select S3.\nChoose Create bucket.\nEnter a Bucket name, for example, webanalytics-huannguyen.\nUncheck \u0026ldquo;Block all public access\u0026rdquo;, then check \u0026ldquo;I acknowledge that the current settings might result in this bucket and the objects within becoming public.\u0026rdquo;.\nKeep the default settings for the rest of the configurations. Choose Create bucket.\nGo to the Properties tab and scroll down.\nUnder Static website hosting, choose Edit.\nEnable Static website hosting. Enter index.html and error.html for Index document and Error document, respectively. Finally, choose Save changes.\nGo to the Permissions tab. Under Bucket policy, choose Edit.\nCopy and paste the following policy. Then, choose Save changes.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Statement1\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::webanalytics-huannguyen/*\u0026#34; } ] } Note: Ensure the Resource field is correctly filled with your bucket\u0026rsquo;s name.\nNext, we will use Visual Studio Code to create an index.html file with the following code: \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Data Visualization with D3.js\u0026lt;/title\u0026gt; \u0026lt;!-- Import D3.js --\u0026gt; \u0026lt;script src=\u0026#34;https://d3js.org/d3.v6.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div style=\u0026#34;width: 80%; margin: auto;\u0026#34;\u0026gt; \u0026lt;!-- SVG element for drawing the chart --\u0026gt; \u0026lt;svg id=\u0026#34;dataChart\u0026#34;\u0026gt;\u0026lt;/svg\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;!-- Data table display --\u0026gt; \u0026lt;div\u0026gt; \u0026lt;h2\u0026gt;Data Table\u0026lt;/h2\u0026gt; \u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;Timestamp\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;Route\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;Count\u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody id=\u0026#34;dataTableBody\u0026#34;\u0026gt;\u0026lt;/tbody\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script\u0026gt; // Function to fetch and process data from the data.json file async function fetchData() { const response = await fetch(\u0026#39;data.json\u0026#39;); const data = await response.json(); // Sort data by timestamp before rendering data.sort((a, b) =\u0026gt; new Date(a.timestamp) - new Date(b.timestamp)); return data; } // Initialize the chart and table async function initChart() { const data = await fetchData(); const width = 800; const height = 400; // Select the SVG element for the chart const svg = d3.select(\u0026#39;#dataChart\u0026#39;) .attr(\u0026#39;width\u0026#39;, width) .attr(\u0026#39;height\u0026#39;, height); // Create a list of timestamps from the data const timestamps = data.map(entry =\u0026gt; new Date(entry.timestamp)); // Create a list of route names from the data const routeNames = Array.from(new Set(data.map(entry =\u0026gt; entry.route))); // Define scaling for the x and y axes const xScale = d3.scaleTime() .domain(d3.extent(timestamps)) .range([60, width - 40]); const yScale = d3.scaleLinear() .domain([0, d3.max(data, d =\u0026gt; d.count)]) .range([height - 40, 20]); // Create data lines for each route for (let i = 0; i \u0026lt; routeNames.length; i++) { const routeData = data.filter(d =\u0026gt; d.route === routeNames[i]); // Define the line generator function const line = d3.line() .x((d, i) =\u0026gt; xScale(new Date(d.timestamp))) .y(d =\u0026gt; yScale(d.count)); // Draw the data line on the chart svg.append(\u0026#39;path\u0026#39;) .datum(routeData) .attr(\u0026#39;fill\u0026#39;, \u0026#39;none\u0026#39;) .attr(\u0026#39;stroke\u0026#39;, d3.schemeCategory10[i]) // Different color for each route .attr(\u0026#39;stroke-width\u0026#39;, 2) .attr(\u0026#39;d\u0026#39;, line); // Display data in the table const dataTableBody = d3.select(\u0026#39;#dataTableBody\u0026#39;); routeData.forEach(entry =\u0026gt; { dataTableBody.append(\u0026#39;tr\u0026#39;) .html(`\u0026lt;td\u0026gt;${entry.timestamp}\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;${entry.route}\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;${entry.count}\u0026lt;/td\u0026gt;`); }); // Add legend for each route svg.append(\u0026#39;circle\u0026#39;) .attr(\u0026#39;cx\u0026#39;, width - 100) .attr(\u0026#39;cy\u0026#39;, 20 + i * 20) .attr(\u0026#39;r\u0026#39;, 6) .attr(\u0026#39;fill\u0026#39;, d3.schemeCategory10[i]); svg.append(\u0026#39;text\u0026#39;) .attr(\u0026#39;x\u0026#39;, width - 90) .attr(\u0026#39;y\u0026#39;, 25 + i * 20) .attr(\u0026#39;font-size\u0026#39;, \u0026#39;12px\u0026#39;) .text(routeNames[i]); } // Add x and y axes const xAxis = d3.axisBottom(xScale) .ticks(5); const yAxis = d3.axisLeft(yScale) .ticks(5); svg.append(\u0026#39;g\u0026#39;) .attr(\u0026#39;transform\u0026#39;, `translate(40, ${height - 40})`) .call(xAxis); svg.append(\u0026#39;g\u0026#39;) .attr(\u0026#39;transform\u0026#39;, `translate(40, 0)`) .call(yAxis); // Add gridlines svg.append(\u0026#39;g\u0026#39;) .attr(\u0026#39;class\u0026#39;, \u0026#39;grid\u0026#39;) .attr(\u0026#39;transform\u0026#39;, `translate(40, 0)`) .call(d3.axisLeft (yScale).ticks(5).tickSize(-width + 60).tickFormat(\u0026#39;\u0026#39;)); svg.append(\u0026#39;g\u0026#39;) .attr(\u0026#39;class\u0026#39;, \u0026#39;grid\u0026#39;) .attr(\u0026#39;transform\u0026#39;, `translate(40, ${height - 40})`) .call(d3.axisBottom(xScale).ticks(5).tickSize(-height + 60).tickFormat(\u0026#39;\u0026#39;)); } // Call the initChart function to initialize the chart and table initChart(); // Automatically refresh the page every 10 seconds setInterval(() =\u0026gt; { location.reload(); }, 10000); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Upload the index.html file you just created to S3: Once the upload is successful, return to the Properties tab and scroll down. You will see the endpoint URL of the website. Click on the link. The website will be displayed as shown below, indicating success. Note: Since there is no data in S3 yet, the web page will be empty, without much information.\n"
},
{
	"uri": "/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "The RealTime Data Pipeline utilizes AWS Kinesis, AWS Lambda, Amazon DynamoDB, Amazon S3 (Simple Storage Service).\nObjectives and Value: The \u0026ldquo;Real-time Data Pipeline\u0026rdquo; project is designed to collect, process, and visualize data from vehicles moving on the road. The project aims to create a flexible, reliable, and scalable system capable of analyzing real-time data, thereby providing significant value to traffic management, road data analysis, and traffic condition forecasting.\nDeployment Plan: The project will be deployed in multiple stages, starting with generating simulated data using Visual Studio Code. Once the data is generated, it will be sent to the Amazon Kinesis stream for collection. Lambda functions will process the data and store it in Amazon DynamoDB. The final processed data will be stored on Amazon S3 and visualized through a web page.\nThe diagram below illustrates the architecture of the data pipeline: Table of Contents Introduction Creating Access Keys Initializing Kinesis Generating Simulated Data Creating Databases Creating Lambda Functions Testing Resource Cleanup "
},
{
	"uri": "/",
	"title": "Serverless Application",
	"tags": [],
	"description": "",
	"content": "Real-time Data Pipeline Deployment Overview In this workshop, we utilize several key services of Amazon Web Services (AWS) to build a system for real-time collection, processing, and visualization of traffic data. Below is an introduction to the services used in the project, along with the concepts and purposes of each service:\nAmazon Kinesis:\nConcept: Amazon Kinesis is a service for real-time data collection, processing, and streaming. Purpose: In the project, Amazon Kinesis is used to receive and collect data from vehicles moving on the road. It enables us to process real-time data and normalize the format before forwarding it to subsequent processing steps. AWS Lambda:\nConcept: AWS Lambda is a serverless compute service that allows you to run code without managing servers. Purpose: In the data pipeline, AWS Lambda is used to process data collected from Kinesis. Upon receiving data, Lambda can perform tasks such as statistical computation, format conversion, data filtering, or complex analytics. Amazon DynamoDB:\nConcept: Amazon DynamoDB is a scalable NoSQL database service that uses key-value storage. Purpose: Data processed by Lambda is stored in Amazon DynamoDB. DynamoDB provides high-performance and scalable storage, suitable for storing and querying real-time traffic data. Amazon S3 (Simple Storage Service):\nConcept: Amazon S3 is a cloud storage service known for its scalability and reliability. Purpose: Processed data is fully stored in Amazon S3. This service allows us to store visualized data, such as charts, graphs, and interactive maps, for easy access and understanding by users. By combining these services, the \u0026ldquo;Real-time Data Pipeline\u0026rdquo; project creates a complete system from real-time data collection to processing and visualization, as well as the deployment timeline of the ETL process.\n"
},
{
	"uri": "/6-lambda/6.2-lambda1/",
	"title": "Creating Lambda 1",
	"tags": [],
	"description": "",
	"content": "Setting Up Lambda Function 1 In this step, we will set up the first Lambda function that will be responsible for processing data from the Kinesis stream and storing it in DynamoDB.\nDetailed Steps: Navigate to the Lambda service in the AWS Console.\nClick Create function.\nConfigure the function as follows:\nFunction name: Enter KinesisToDynamoDB. Runtime: Choose Python 3.9. Use an existing role: Select the role you created earlier (e.g., LambdaRolesWorkshop). Click Create function. Copy and paste the following code into the editor. Then click Deploy.\nimport boto3 import binascii import json dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) table_name = \u0026#39;Data-Kinesis\u0026#39; # Replace with your DynamoDB table name table = dynamodb.Table(table_name) def lambda_handler(event, context): print(event) for record in event[\u0026#39;Records\u0026#39;]: payload = binascii.a2b_base64(record[\u0026#39;kinesis\u0026#39;][\u0026#39;data\u0026#39;]) data = json.loads(payload) response = table.put_item( Item={ \u0026#39;timestamp\u0026#39;: data[\u0026#39;timestamp\u0026#39;], \u0026#39;route\u0026#39;: data[\u0026#39;route\u0026#39;], \u0026#39;count\u0026#39;: data[\u0026#39;count\u0026#39;] } ) print(f\u0026#34;Stored data in DynamoDB: {response}\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Data stored in DynamoDB\u0026#39;) } Scroll down to the Add a layer section.\nChoose Specify an ARN, and enter arn:aws:lambda:ap-southeast-1:770693421928:layer:Klayers-p39-boto3:18. Then click Verify and Add.\nClick Add trigger.\nSelect Kinesis as the trigger type. Choose the Kinesis stream you created earlier. Set Starting position to Trim horizon. Then click Add.\nWait for a minute until the trigger status becomes Enabled.\nTesting Lambda Function 1 Open Visual Studio Code and run the createData.py script you created earlier.\nGo back to the Lambda function page, navigate to the Monitor tab, and click on View CloudWatch logs.\nIn the new window, select the latest log stream. If the logs are not yet visible, wait a minute and click Reload.\nYou should see that there are no error messages.\nAccess the DynamoDB table (Data-Kinesis) you created earlier. Click Explore table items.\nYou can see that the data sent from Visual Studio Code has been successfully stored in the table.\nNow you have successfully set up Lambda function 1 to process data from the Kinesis stream and store it in DynamoDB.\n"
},
{
	"uri": "/2-accesskeys/",
	"title": "Initializing Access Keys",
	"tags": [],
	"description": "",
	"content": "Initializing Access Keys In this workshop, we will need to interact with AWS using the Command Line Interface (CLI). Therefore, to establish a connection, we need to create Access Keys for this purpose.\nInitialization Process: On the console page, select your account name in the upper right corner. Then choose Security credentials. Select Create access key. Check the box \u0026ldquo;I understand creating a root access key is not a best practice, but I still want to create one.\u0026rdquo;. Then choose Create access key. Here, the Access Key - Secret access key information is displayed. We can download it (Download .csv file). Then choose Save. That\u0026rsquo;s it, we have successfully initialized it.\n"
},
{
	"uri": "/6-lambda/6.3-lambda2/",
	"title": "Creating Lambda 2",
	"tags": [],
	"description": "",
	"content": "Detailed Steps: Similarly to creating Lambda 1, navigate to the Lambda service page and select Create function.\nEnter a Function name as DynamoDBToS3. Choose Python 3.9 as the Runtime. Select Use an existing role and choose the role you created earlier, RoleLab2. Finally, click Create function.\nCopy and paste the following code into the editor. Then, click Deploy. import boto3 import json from datetime import datetime, timedelta, timezone # Initialize DynamoDB and S3 clients dynamodb = boto3.client(\u0026#39;dynamodb\u0026#39;) s3_client = boto3.client(\u0026#39;s3\u0026#39;) def get_data_from_dynamodb(start_time, end_time): # Retrieve all data from DynamoDB response = dynamodb.scan( TableName=\u0026#39;Data-Kinesis\u0026#39; ) # Filter data within the specified time range filtered_data = [] for item in response[\u0026#39;Items\u0026#39;]: timestamp = item[\u0026#39;timestamp\u0026#39;][\u0026#39;S\u0026#39;] count = item[\u0026#39;count\u0026#39;][\u0026#39;N\u0026#39;] route = item[\u0026#39;route\u0026#39;][\u0026#39;S\u0026#39;] if start_time \u0026lt;= timestamp \u0026lt;= end_time: filtered_data.append({ \u0026#34;timestamp\u0026#34;: timestamp, \u0026#34;count\u0026#34;: int(count), \u0026#34;route\u0026#34;: route }) return filtered_data def upload_json_to_s3(data): # Save data to a JSON file in memory json_data = json.dumps(data) # Upload the JSON file to Amazon S3 s3_client.put_object( Bucket=\u0026#39;webanalytics-huannguyen\u0026#39;, Key=\u0026#39;data.json\u0026#39;, # Path and filename on S3 Body=json_data.encode(\u0026#39;utf-8\u0026#39;) ) def lambda_handler(event, context): # Get current time and the time one hour ago in Vietnamese timezone (GMT+7) vn_timezone = timezone(timedelta(hours=7)) end_time = (datetime.now(vn_timezone)).strftime(\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;) start_time = (datetime.now(vn_timezone) - timedelta(hours=1)).strftime(\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;) # Get data from DynamoDB data = get_data_from_dynamodb(start_time, end_time) # Upload JSON data to Amazon S3 upload_json_to_s3(data) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Data updated and uploaded to S3 successfully.\u0026#39;) } Note: The above code fetches the project 1 hour before current time from DynamoDB and saves it to data.json in S3 container\nScroll down to the bottom of the page and select Add a layer. Choose Specify an ARN and enter arn:aws:lambda:ap-southeast-1:770693421928:layer:Klayers-p39-boto3:18. Then, click Verify and Add. Select Add trigger. Choose DynamoDB and then select the DynamoDB table you created earlier. Choose Latest as the Starting position. Finally, click Add. Wait for about a minute until the status changes to Enabled. "
},
{
	"uri": "/3-kinesis/",
	"title": "Initializing Kinesis",
	"tags": [],
	"description": "",
	"content": "In this section, we will proceed to initialize a data stream in Amazon Kinesis, laying the foundation for real-time data collection and processing from various sources.\nImplementation Steps: Enter Kinesis in the service search bar on the AWS Console, then select Kinesis. Choose Create data stream. In the Data stream name section, enter KinesisDataStream. Then select Provisioned and input 1. Finally, choose Create data stream. Wait for about 2 minutes, and when the status changes to Active, the setup is complete. "
},
{
	"uri": "/4-ide/",
	"title": "Creating Simulated Data",
	"tags": [],
	"description": "",
	"content": "In this workshop, since we do not have direct data from sensors, we will use Visual Studio Code to generate simulated data regarding the number of vehicles traveling on four different routes: \u0026ldquo;Route A,\u0026rdquo; \u0026ldquo;Route B,\u0026rdquo; \u0026ldquo;Route C,\u0026rdquo; and \u0026ldquo;Route D.\u0026rdquo; We will then send this data to the Kinesis data stream to continue the processing and analysis.\nNote: The data is sent in JSON format as follows: { \u0026ldquo;timestamp\u0026rdquo;: \u0026ldquo;2023-08-15 19:22:02\u0026rdquo;, \u0026ldquo;route\u0026rdquo;: \u0026ldquo;Route B\u0026rdquo;, \u0026ldquo;count\u0026rdquo;: 75 }\nImplementation Steps: Part 1 - Sending Data Open Visual Studio Code and create a new file named createData.py. Then open the Terminal to establish a connection with AWS using the previously created Access Key.\nUse the command: aws configure Fill in the Access Key - Secret access key - region name - output format one by one. Access Key - Secret access key: Get this information from Part 2. region name: The region you are working in, such as ap-southeast-1. output format: json Copy and paste the following code into the newly created file.\nimport random import json import boto3 import time from datetime import datetime # Create a connection to Kinesis Data Streams client = boto3.client(\u0026#39;kinesis\u0026#39;, region_name=\u0026#39;ap-southeast-1\u0026#39;) stream_name = \u0026#39;demo\u0026#39; # List of routes routes = [\u0026#34;Route A\u0026#34;, \u0026#34;Route B\u0026#34;, \u0026#34;Route C\u0026#34;, \u0026#34;Route D\u0026#34;] # Function to generate simulated traffic events for all routes def generate_traffic_event(route): count = random.randint(30, 100) # Number of vehicles on the route, 30-100 timestamp = datetime.now().strftime(\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;) event_data = { \u0026#39;timestamp\u0026#39;: timestamp, \u0026#39;route\u0026#39;: route, \u0026#39;count\u0026#39;: count } return json.dumps(event_data) # Send simulated data to the data stream for all routes while True: for route in routes: event_data = generate_traffic_event(route) response = client.put_record( StreamName=stream_name, Data=bytes(event_data, \u0026#39;utf-8\u0026#39;), PartitionKey=\u0026#39;123\u0026#39; ) print(f\u0026#34;Sent: {event_data}\u0026#34;) time.sleep(1) time.sleep(30) Click the triangle icon in the upper-right corner (Run code) to execute it. You will see notifications of the sent data in the terminal window. Part 2 - Checking Data Sent to Kinesis: Open the Kinesis service. Choose the Data Streams that we created earlier. Go to the Data viewer tab. Select Shard as ShardId-00000000000, choose Trim horizon for Starting position, and then click Get records. Here, you will see the 4 lines of data that were sent earlier. This indicates that Kinesis has received the data.\n"
},
{
	"uri": "/5-databases/",
	"title": "Initializing Databases",
	"tags": [],
	"description": "",
	"content": "In this workshop, we will leverage two essential storage services from Amazon Web Services (AWS): Amazon DynamoDB and Amazon S3.\nDynamoDB is used to store processed data from Amazon Kinesis. It\u0026rsquo;s a scalable NoSQL database that enables efficient data storage and querying, supporting complex data management and real-time data analytics.\nS3 is used to store processed and visualized data from the project. Amazon S3 is a cloud storage service with scalability, allowing you to store various types of data such as files, images, videos, and structured data. This data can be easily accessed and shared through links and integrated with other services and applications on the AWS platform.\n"
},
{
	"uri": "/6-lambda/",
	"title": "Initializing Lambda Functions",
	"tags": [],
	"description": "",
	"content": "In this section, we will perform three important steps to build a real-time data flow:\nInitialize Roles for Lambda Functions: First, we will create a specialized role for the Lambda Functions. This role will provide the necessary permissions for Lambda to safely and securely interact with other services such as Kinesis, DynamoDB, and S3.\nConfigure Lambda 1: Sending Data from Kinesis to DynamoDB: Next, we will configure the first Lambda Function. This Lambda will be responsible for taking data from the Kinesis stream we created earlier and storing it in DynamoDB. This process will handle and standardize the data for easier querying and analysis later.\nConfigure Lambda 2: Sending Data from DynamoDB to S3 and Storing as JSON: Finally, we will configure the second Lambda Function. This Lambda will retrieve the processed data stored in DynamoDB and send it to Amazon S3. The data will be stored as JSON files in S3, providing a foundation for data visualization and analysis.\nThese three steps together create a complete workflow from real-time data collection to storage and visualization.\nContents Creating Roles Creating Lambda 1 Creating Lambda 2 "
},
{
	"uri": "/7-test/",
	"title": "Testing the Solution",
	"tags": [],
	"description": "",
	"content": "Testing the Solution Start by opening Visual Studio Code (VS Code) and running the createData.py script. This script contains the code to generate mock data for testing. By running this script, we will generate data related to traffic flow on different routes.\nNext, open the web page that we configured earlier. This web page will allow us to visualize the data in a graphical and intuitive manner. You will notice that after data is sent from VS Code, the web page will automatically update and display the visualized data.\nThis process enables you to observe the entire workflow from generating mock data to visualizing real-time data through the AWS services we configured.\nThe result after running continuously for 10 minutes. "
},
{
	"uri": "/8-terminate/",
	"title": "Cleaning Up Resources",
	"tags": [],
	"description": "",
	"content": "Cleaning Up Resources Before concluding this workshop, it\u0026rsquo;s important to clean up the resources to avoid incurring unnecessary charges.\nS3 Access S3 and select the S3 bucket that was created during this lab. Then, choose Empty. Enter permanently delete and choose Empty. Select the S3 bucket and choose Delete. Enter the bucket name webanalytics-huannguyen and choose Delete bucket. Lambda Access Lambda and select the Lambda functions that were created during this lab. Then, choose Action \u0026ndash;\u0026gt; Delete. Enter delete and choose Delete. DynamoDB Access DynamoDB and select the tables that were created during this lab. Then, choose Delete. Enter confirm and choose Delete. Kinesis Access Kinesis and select the Data stream that was created during this lab. Then, choose Action \u0026ndash;\u0026gt; Delete. Enter delete and choose Delete. Role Access IAM, go to the Role tab, select the role that was created during this lab, and then choose Delete. Enter the role name RoleLab2 and choose Delete. Access Keys In the AWS Management Console, select your account name in the top right corner. Then choose Security credentials. Select the Access Key that was created during this lab. Then, choose Action \u0026ndash;\u0026gt; Delete. Choose Deactivate. Enter the Access Key name and choose Delete. By following these steps, you have successfully cleaned up all the resources used in this workshop. Thank you for your participation and attention.\n"
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]